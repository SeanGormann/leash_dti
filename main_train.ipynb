{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import polars as pl\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "#from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "\n",
    "from models import *\n",
    "from data import *\n",
    "from utils import *\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path_ = \"data/\"\n",
    "filenames = os.listdir(os.path.join(root_path_, 'graph_dataset_11m'))\n",
    "filenames = [f\"graph_data_v11/{filename}\" for filename in filenames if filename.endswith(\".pt\")]\n",
    "\n",
    "\"\"\"\n",
    "pos_filenames = os.listdir(os.path.join(root_path_, 'positives'))\n",
    "pos_filenames = [f\"positives/{filename}\" for filename in pos_filenames if filename.endswith(\".pt\")]\n",
    "neg_filenames = os.listdir(os.path.join(root_path_, 'negatives_2ndhalf'))\n",
    "neg_filenames = [f\"negatives_2ndhalf/{filename}\" for filename in neg_filenames if filename.endswith(\".pt\")]\n",
    "\n",
    "path_to_blind = \"data/all_blind_mols_half\"\n",
    "blind_filenames = os.listdir(path_to_blind)\n",
    "blind_filenames = [filename for filename in blind_filenames if filename.endswith(\".pt\")]\n",
    "\"\"\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "TESTING = True\n",
    "\n",
    "if TESTING:\n",
    "    train_dataset = DynamicGraphDataset(root_path_, filenames, fold=0, nfolds=2, train=True, device=device, active_batches=1)\n",
    "    val_dataset = DynamicGraphDataset(root_path_, filenames, fold=0, nfolds=2, train=False, device=device, active_batches=1)\n",
    "else:\n",
    "    train_dataset = DynamicGraphDataset(root_path_, filenames, fold=0, nfolds=15, train=True, device=device, active_batches=28)\n",
    "    val_dataset = DynamicGraphDataset(root_path_, filenames, fold=0, nfolds=15, train=False, device=device, active_batches=2)\n",
    "\"\"\"\n",
    "if TESTING:\n",
    "    train_dataset = GraphDataset(root_path_, pos_filenames[:1], neg_filenames[:1], eval=False, device = device) #, oversample_factor=2) #use_half_pos=True)\n",
    "    blind_dataset = GraphDataset(path_to_blind, blind_filenames[:1], None, eval=True, device = device)\n",
    "else:\n",
    "    train_dataset = GraphDataset(root_path_, pos_filenames, neg_filenames, eval=False, device = device) #, oversample_factor=2) # use_half_pos=True)\n",
    "    blind_dataset = GraphDataset(path_to_blind, blind_filenames, None, eval=True, device = device)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "dl_train = GeoDataLoader(train_dataset, batch_size=256, shuffle=True) #, persistent_workers=True, num_workers=4)\n",
    "dl_val = GeoDataLoader(val_dataset, batch_size=256, shuffle=True) #, persistent_workers=False, num_workers=2)\n",
    "\n",
    "print(f\"Number of training batches: {len(dl_train)}, Number of validation batches: {len(dl_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiMolGNN(num_node_features=8, num_edge_features=4, num_layers=8, hidden_dim=320, dropout=0.1)\n",
    "#model = MolGraphTransformer(n_layers=4, node_dim=8, edge_dim=4, hidden_dim=320, n_heads=8, in_feat_dropout=0.1, dropout=0.1, pos_enc_dim=4)\n",
    "model.to(device)\n",
    "print(f'Number of parameters: {sum(p.numel() for p in model.parameters())}')\n",
    "\n",
    "\n",
    "# Total number of epochs and warmup steps\n",
    "n_epochs = 15\n",
    "warmup_steps = 196 #196\n",
    "total_steps = len(train_dataset) * n_epochs\n",
    "initial_lr = 0.0001 #0.0001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr) #lr=0.0001)\n",
    "#criterion = nn.BCEWithLogitsLoss(pos_weight=2)\n",
    "weights = torch.tensor([1.0, 2.0])\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=weights[1]) \n",
    "criterion = FocalLoss(alpha=0.5, gamma=2.0)\n",
    "\n",
    "# Scheduler and scaler for AMP\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=n_epochs - 1, eta_min=0)\n",
    "#scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n",
    "#scheduler = ReduceLROnPlateau(optimizer, 'max', factor=0.5, patience=2)\n",
    "scaler = GradScaler()\n",
    "\n",
    "protein_index = {'BRD4': 0, 'HSA': 1, 'sEH': 2}  # Default to 0th index if protein not found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(enumerate(dl_train), total=len(dl_train), desc=f\"Epoch {epoch + 1}/{n_epochs}\")\n",
    "    for i, batch in pbar:\n",
    "        # Warm-up phase\n",
    "        if epoch == 0 and i < warmup_steps:\n",
    "            lr = initial_lr * (i + 1) / warmup_steps\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "        else:\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        _, _, loss = process_batch(batch.to(device), model, scaler, optimizer, criterion, train=True, protein_index=None) #protein_index['HSA']\n",
    "        total_loss += loss\n",
    "        pbar.set_postfix(loss=f\"{total_loss / (i + 1):.4f}\", lr=f\"{lr:.6f}\")\n",
    "\n",
    "    # Step the scheduler only after warm-up is complete\n",
    "    scheduler.step()\n",
    "\n",
    "    # Validation and Metric Calculation\n",
    "    model.eval()\n",
    "    model_outputs = []\n",
    "    true_labels = []\n",
    "    vloss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dl_val:\n",
    "            outputs, labels, val_loss = process_batch(batch.to(device),  model, scaler, optimizer, criterion, train=False, protein_index=None)\n",
    "            model_outputs.append(outputs.squeeze().cpu())\n",
    "            true_labels.append(labels.cpu())\n",
    "            vloss += val_loss\n",
    "\n",
    "    model_outputs = torch.cat(model_outputs)\n",
    "    true_labels = torch.cat(true_labels)\n",
    "    model_outputs = torch.sigmoid(model_outputs).numpy()  # Apply sigmoid to convert logits to probabilities\n",
    "\n",
    "    #train_dataset.shuffle_files()\n",
    "    #train_dataset.update_active_set()\n",
    "    #dl_train = GeoDataLoader(train_dataset, batch_size=196, shuffle=True)\n",
    "    # Calculate Mean Average Precision (micro)\n",
    "    map_micro = average_precision_score(true_labels.numpy(), model_outputs, average='micro')\n",
    "    #print(map_micro)\n",
    "    #scheduler.step(map_micro)\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {total_loss / len(dl_train)}, Val Loss: {vloss/len(dl_val)},Validation MAP (micro): {map_micro}, Time: {time.time() - start_time}\\n----\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path and filename\n",
    "model_path = 'models/multi_model_v17.pth'\n",
    "\n",
    "# Save the model's state dictionary\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
